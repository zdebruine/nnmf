% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/als_nmf.R
\name{als_nnmf}
\alias{als_nnmf}
\title{Non-negative Matrix Factorization by Alternating Least Squares}
\usage{
als_nnmf(
  data,
  k,
  test_size = 0.0615,
  test_seed = 129,
  seed = 42,
  tol = 1e-04,
  epochs = 100,
  verbose = TRUE,
  L1 = c(0, 0),
  L2 = c(0, 0),
  ortho = c(0, 0),
  log = NULL,
  num_threads = 0
)
}
\arguments{
\item{data}{Dense or sparse matrix}

\item{k}{rank}

\item{test_size}{Integer giving the proportion of values that will be reserved for the test set in a random speckled pattern. This proportion will be inverted and rounded to the nearest integer for efficient RNG purposes.}

\item{test_seed}{Random seed determining the test set.}

\item{seed}{A number giving the random seed for initializing W, or a matrix giving initial W of dimensions m x k}

\item{tol}{stopping criterion giving the relative 2-norm distance between W across consecutive iterations at which to call convergence}

\item{epochs}{maximum number of alternating least squares updates for which to fit}

\item{verbose}{Print logging output to console}

\item{L1}{L1 penalty, optionally a vector of two giving penalty on c(W, H) individually}

\item{L2}{L2 penalty, optionally a vector of two giving penalty on c(W, H) individually}

\item{ortho}{orthogonality penalty, optionally a vector of two giving penalty on c(W, H) individually}

\item{log}{Parameters that are nearly free to compute will be automatically logged. In addition, you may specify any of c("test_loss", "train_loss")}

\item{num_threads}{Number of threads to use for parallelization. If 0, will use all available threads.}
}
\description{
This is the classical NMF algorithm implemented in-core that requires transposition of the input data.
}
\details{
Factors in W and H are normalized to their 2-norm after each iteration.

The input matrix will be copied to float type on the C++ back-end. This requires additional RAM, but provides significant speedups.

Training set reconstruction error is not computed during model fitting. If it is desired, runtime may increase significantly.

Masking the test set significantly decreases scalability of the algorithm, which will be noticeable at moderate ranks (50-200) and will begin to render models computationally intractable (appx. k > 500)

By default, W is initialized with random normal values. H is first solved given the input data and W, and thus is not initialized. You may provide a custom initialization for W if desired, but NNDSVD is not recommended due to the local minimum it already supplies that makes it difficult for NMF to discover a truly optimal solution.
}
